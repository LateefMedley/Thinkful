{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: evaluate your sentiment classifier\n",
    "\n",
    "It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "    * Do any of your classifiers seem to overfit?\n",
    "    * Which seem to perform the best? Why?\n",
    "    * Which features seemed to be most impactful to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell imports the dataset of interest, with 2 columns \"review\" and \"sentiment.\" I am interested in predicting sentiment based on the \"review.\"\n",
    "\n",
    "Following the import, I create a keywords list and iterate through that list to create a new column in the data frame for each keyword in the list and the values in each observation relating to that column is a boolean that tells us whether that corresponding keyword is in that observation's review.\n",
    "\n",
    "After that, I declare the data (keywords) and target (sentiment).\n",
    "\n",
    "Next, I load the model with the data/target and create a new variable to store the model's results.\n",
    "\n",
    "Finally, I print overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Orginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 748 points : 281\n",
      "Our sensitivty is: 0.9016\n",
      "Our specificity is: 0.3287\n",
      "\n",
      " Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[119, 243],\n",
       "       [ 38, 348]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r'C:\\Users\\latee\\Downloads\\sentiment labelled sentences\\sentiment labelled sentences\\imdb_labelled.txt'\n",
    "\n",
    "IMDB = pd.read_csv(PATH, delimiter= '\\t', header=None)\n",
    "IMDB.columns = ['review', 'sentiment']\n",
    "\n",
    "keywords = ['bad', 'funny', 'best', 'great', 'terrible', 'love', 'good', 'plot', 'never', 'real', 'really', 'script',\n",
    "           'one', 'actor', 'see', 'little', 'make', 'way', 'recommend', 'line', 'movie', 'film', 'acting', 'even',\n",
    "           'scene', 'watching', 'excellent', 'seen', 'piece', 'say', 'show', 'dialogue', 'perfect', 'cheap', 'thing']\n",
    "\n",
    "for key in keywords:\n",
    "    IMDB[str(key)] = IMDB.review.str.contains(' ' + str(key) + ' ', case=False)\n",
    "\n",
    "data = IMDB[keywords]\n",
    "target = IMDB['sentiment']\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(data, target)\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], (target != y_pred).sum()))\n",
    "print(\"Our sensitivty is: {}\".format(round(348/386, 4)))\n",
    "print(\"Our specificity is: {}\".format(round(119/362, 4)))\n",
    "print(\"\\n Confusion Matrix\")\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now to check for overfitting and accuracy using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(bnb, data, target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 748 points : 328\n",
      "Our sensitivty is: 0.9741\n",
      "Our specificity is: 0.1215\n",
      "\n",
      " Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 44, 318],\n",
       "       [ 10, 376]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['bad', 'terrible', 'never', 'cheap']\n",
    "\n",
    "for key in keywords:\n",
    "    IMDB[str(key)] = IMDB.review.str.contains(' ' + str(key) + ' ', case=False)\n",
    "\n",
    "data = IMDB[keywords]\n",
    "target = IMDB['sentiment']\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(data, target)\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], (target != y_pred).sum()))\n",
    "print(\"Our sensitivty is: {}\".format(round(376/386, 4)))\n",
    "print(\"Our specificity is: {}\".format(round(44/362, 4)))\n",
    "print(\"\\n Confusion Matrix\")\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(bnb, data, target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Positive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 748 points : 323\n",
      "Our sensitivty is: 0.2228\n",
      "Our specificity is: 0.9365\n",
      "\n",
      " Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[339,  23],\n",
       "       [300,  86]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['funny', 'best', 'great', 'love', 'good', 'recommend', 'excellent','perfect']\n",
    "\n",
    "for key in keywords:\n",
    "    IMDB[str(key)] = IMDB.review.str.contains(' ' + str(key) + ' ', case=False)\n",
    "\n",
    "data = IMDB[keywords]\n",
    "target = IMDB['sentiment']\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(data, target)\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], (target != y_pred).sum()))\n",
    "print(\"Our sensitivty is: {}\".format(round(86/386, 4)))\n",
    "print(\"Our specificity is: {}\".format(round(339/362, 4)))\n",
    "print(\"\\n Confusion Matrix\")\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(bnb, data, target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Positive + Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 748 points : 325\n",
      "Our sensitivty is: 0.987\n",
      "Our specificity is: 0.116\n",
      "\n",
      " Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 42, 320],\n",
       "       [  5, 381]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['funny', 'best', 'great', 'love', 'good', 'recommend', 'excellent','perfect',\n",
    "           'bad', 'terrible', 'never', 'cheap']\n",
    "\n",
    "for key in keywords:\n",
    "    IMDB[str(key)] = IMDB.review.str.contains(' ' + str(key) + ' ', case=False)\n",
    "\n",
    "data = IMDB[keywords]\n",
    "target = IMDB['sentiment']\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(data, target)\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], (target != y_pred).sum()))\n",
    "print(\"Our sensitivty is: {}\".format(round(381/386, 4)))\n",
    "print(\"Our specificity is: {}\".format(round(42/362, 4)))\n",
    "print(\"\\n Confusion Matrix\")\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(bnb, data, target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the above 4 models, the original one performs best. \n",
    "\n",
    "Even though the original model is better at detecting negative sentiment than it is at a positive one, it's cross-validation results tells me that it is most consistently more reliable than the other three. One of the biggest distinctions between the three models after the original one is the number of keywords used in data to detect our target. \n",
    "\n",
    "For the Fifth Model, I will stuff a ton more keywords into our data to see if we gain accuracy and uncover the costs of doing so. And since there doesn't seem to be overfitting in any of the above models this seems like a worthwhile tactic to try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - Word Blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 748 points : 270\n",
      "Our sensitivty is: 0.8782\n",
      "Our specificity is: 0.384\n",
      "\n",
      " Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[139, 223],\n",
       "       [ 47, 339]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['bad', 'funny', 'best', 'great',\n",
    "            'terrible', 'love', 'good', 'plot',\n",
    "            'never', 'real', 'really', 'script',\n",
    "           'one', 'actor', 'see', 'little', 'make',\n",
    "            'way', 'recommend', 'line', 'movie', 'film','acting', 'even',\n",
    "           'scene', 'watching', 'excellent', 'seen', 'piece',\n",
    "            'say', 'show', 'dialogue', 'perfect', 'cheap', 'thing',\n",
    "           'give', 'work', 'go', 'horror', 'life', 'found', \n",
    "           'kid', 'mess', 'fan', 'budget', 'song', 'lack', 'face',\n",
    "           'effect', 'time', 'much', 'truly', 'black', 'tv', 'human', \n",
    "           'ending', 'garbage', 'flick', 'casting']\n",
    "\n",
    "for key in keywords:\n",
    "    IMDB[str(key)] = IMDB.review.str.contains(' ' + str(key) + ' ', case=False)\n",
    "\n",
    "data = IMDB[keywords]\n",
    "target = IMDB['sentiment']\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(data, target)\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], (target != y_pred).sum()))\n",
    "print(\"Our sensitivty is: {}\".format(round(339/386, 4)))\n",
    "print(\"Our specificity is: {}\".format(round(139/362, 4)))\n",
    "print(\"\\n Confusion Matrix\")\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(bnb, data, target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model is lackluster as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Which features seemed to be most impactful to performance?\n",
    "    \n",
    "During my time spent in this notebook, I discovered that the features used have a huge impact on performance. Even though the accuracy produced by all of these models are not what I would consider reliable, the keywords used highly influenced the type errors produced by each model. The key models of this discovery are the negative word and positive word models. \n",
    "\n",
    "The negative model is more susceptible to type 1 error, alternatively, we can say that the model is more likely to assume positive reviews are going to be negative. The negative words model is a lot better at finding negative reviews but did not perform so well when classifying positive reviews. \n",
    "\n",
    "The positive model is more susceptible to type 2 errors. The positive words model is a lot better at finding positive reviews but performs poorly when faced with negative ones. \n",
    "\n",
    "More concisely put, the negative model has high sensitivity and low specificity. The inverse is true for the positive model. \n",
    "\n",
    "And although the accuracy of all the models generated was not \"acceptable,\" there is something to be said about the tightness of variance in the resulting accuracy. So at least we don't have some overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
